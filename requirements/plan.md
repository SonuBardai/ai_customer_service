# Technical Architecture and Development Plan for Customer Support AI Chatbot Platform

## Overview and Goals 
This platform is a multi-tenant SaaS solution that lets companies deploy AI chatbots for customer support. Each company (tenant) can sign up, configure a custom chatbot (with their own knowledge base and tone), and embed the chatbot on their website. Key goals include:
- **Accurate Support Q&A:** The chatbot should answer questions using only the company’s uploaded support content (retrieval-augmented generation) ([How We Built a Domain-Specific Support Chatbot Using Firebolt with RAG](https://www.firebolt.io/blog/how-we-built-a-domain-specific-support-chatbot-using-firebolt-with-rag#:~:text=TL%3BDR%3A%20Firebolt%20built%20a%20support,latency%20answers)).
- **Fast, Interactive UX:** The widget must load quickly (<1s) and support multi-turn conversations with low latency (streaming answers).
- **Easy Integration:** Companies get a snippet to embed the chatbot on their site, matching their branding.
- **Security & Isolation:** Ensure each company’s data is isolated and enforce usage limits, origin checks, and prompt safety.

## System Architecture 
**Architecture Overview:** The system follows a typical web SaaS architecture, composed of a backend API server, a frontend web app (for admin/configuration), a database, and an external AI service. Below is a text-based “diagram” of the main components and data flow:

- **Web Frontend (Admin Dashboard)** – A TypeScript React application (deployed on Vercel) that company staff use to sign up, configure the chatbot (upload knowledge base, set tone/branding), and view analytics. It communicates with the backend via HTTPS REST API calls.
- **Backend API (Django on AWS)** – A Python Django server hosted on AWS (e.g. EC2 or container service). It exposes RESTful endpoints for all functionality (auth, bot configuration, chat queries, analytics). The backend orchestrates data between the frontend, database, and AI model. It ensures multi-tenant isolation by scoping all data queries to the company’s ID ([Evolving django-multitenant to build scalable SaaS apps on Postgres & Citus - Citus Data](https://www.citusdata.com/blog/2023/05/09/evolving-django-multitenant-to-build-scalable-saas-apps-on-postgres-and-citus/#:~:text=Citus%20open%20source%20or%20on,multitenant%20easy%20for%20you)).
- **Firebase NoSQL Database** – A Firebase Firestore database stores all persistent data: user accounts (or Firebase Auth), company profiles, chatbot configurations, knowledge base documents, and conversation logs. Data is partitioned by company/tenant to prevent any cross-tenant access (each record is tagged with a tenant ID for filtering on every query ([Evolving django-multitenant to build scalable SaaS apps on Postgres & Citus - Citus Data](https://www.citusdata.com/blog/2023/05/09/evolving-django-multitenant-to-build-scalable-saas-apps-on-postgres-and-citus/#:~:text=Citus%20open%20source%20or%20on,multitenant%20easy%20for%20you))).
- **AI LLM Service** – The chatbot’s brain is an external Large Language Model (e.g. OpenAI GPT-4 API). The backend uses a retrieval-augmented generation approach: it searches the company’s knowledge base for relevant info and feeds that context to the LLM to generate answers ([How We Built a Domain-Specific Support Chatbot Using Firebolt with RAG](https://www.firebolt.io/blog/how-we-built-a-domain-specific-support-chatbot-using-firebolt-with-rag#:~:text=TL%3BDR%3A%20Firebolt%20built%20a%20support,latency%20answers)). The LLM is invoked with a *system prompt* that includes the company’s tone/persona instructions and retrieved knowledge, plus the conversation history.
- **Embeddable Chat Widget** – A lightweight JavaScript widget that runs on the company’s own website. This widget (loaded from the backend or CDN) presents the chat UI to end-users. It communicates with the backend API (over HTTPS) to send user questions and receive streaming answers. The widget can be embedded on approved domains only and will match the company’s branding.

**Request–Response Flow:** When an end-user asks a question in the embedded chat, the flow is:
1. **User Query:** The user’s browser (on the company’s site) sends the question via the widget script to the backend API (e.g. a `/api/chat` endpoint). The request includes the company’s ID (or API key) so the backend knows which tenant’s bot to invoke, and a conversation/session identifier for context.
2. **Retrieval:** The Django backend receives the query. It verifies the request (authentication, rate limiting, domain check) then fetches relevant support content from Firebase Firestore. For example, it might perform a vector similarity search on the company’s knowledge base documents to find the most relevant pieces ([How We Built a Domain-Specific Support Chatbot Using Firebolt with RAG](https://www.firebolt.io/blog/how-we-built-a-domain-specific-support-chatbot-using-firebolt-with-rag#:~:text=TL%3BDR%3A%20Firebolt%20built%20a%20support,latency%20answers)) (see *AI Integration* below).
3. **LLM Prompting:** The backend constructs a prompt for the LLM: it injects a fixed system message that establishes the chatbot’s role and tone (e.g. *“You are a helpful support assistant for Company X, speaking in a friendly tone...”*), provides the retrieved knowledge context, and appends the conversation history + new question. The LLM API is called (via HTTP request) with `stream=True` so that tokens are returned incrementally.
4. **Streamed Response:** As the LLM generates an answer, the Django backend streams the partial response back to the client. The backend uses Server-Sent Events (SSE) or a similar mechanism (Django’s `StreamingHttpResponse`) to flush chunks of data as they arrive ([Stream AI chats using Django in 5 minutes (OpenAI and Anthropic)  - Tom Dekan](https://tomdekan.com/articles/llm-stream#:~:text=No%20third)). The widget receives these chunks and updates the chat UI in real-time (the user sees the answer appear word-by-word).
5. **Fallback & Completion:** If the LLM indicates no answer (or confidence is low), the backend/widget can trigger a fallback – e.g. asking the user for their email so a human can follow up. In all cases, the full Q&A exchange is saved to Firestore as a conversation log. The company’s dashboard can later display this log for review or analytics.

**Admin Flow:** Separately, company staff interact with the admin React app:
- They create an account or log in, which authenticates with the backend (possibly using Firebase Auth or Django Auth).
- After login, they create or select their “Company” profile (if the platform supports multiple projects per account). They then upload support documents or FAQs, configure tone/voice settings, and generate the embed snippet. Each of these actions calls the backend (e.g. an endpoint to upload a document, which the backend stores in Firestore and also pre-computes its embedding vector for search).
- The dashboard may also allow testing the chatbot live and viewing analytics like number of conversations, unanswered questions, etc., fetched from the backend.

## Technology Stack and Rationale 

| **Component**            | **Technology**                 | **Rationale**                                             |
|--------------------------|-------------------------------|-----------------------------------------------------------|
| **Backend API**          | **Python Django** on AWS      | Django provides a robust, secure framework for rapid development of web APIs and multi-tenant apps. It has built-in admin, ORM, and authentication which accelerate development. Python’s rich ecosystem (e.g. for AI integration) makes it easier to call AI APIs and process data. Deploying on AWS ensures scalability (using EC2, ECS, or Elastic Beanstalk) and integrates well with other AWS services if needed (S3 for file storage, etc.). |
| **Frontend Web App**     | **React + TypeScript** on Vercel | React is a popular library for building dynamic UIs. Using TypeScript helps catch errors early and improve code quality. Vercel is a great choice for deployment because it can host the React app (or Next.js app) globally with CDN caching, ensuring quick load times. Vercel’s infrastructure also simplifies CI/CD – every push can auto-deploy. The frontend will be a single-page application for the admin dashboard, making the UX smooth. |
| **Embeddable Widget**    | **Vanilla JS/React (bundled)** served via CDN | The chat widget might be built with lightweight React or plain JavaScript, then bundled into a minified script. It’s served via a CDN or directly from Vercel (for global edge delivery). This ensures that when the snippet is placed on a client’s site, it loads in under 1 second. Using the same tech (React/TS) for the widget means code reusability from the main app (for instance, styling or state management logic). |
| **Database (NoSQL)**     | **Firebase Firestore**        | Firestore is a cloud-hosted NoSQL database that scales automatically and requires no server management. It stores JSON-like documents, which is well-suited for flexible data like knowledge base articles and chat logs. Firestore also provides real-time listeners (though for our use-case, we mostly use it as a standard DB via the backend). Using Firebase simplifies initial MVP setup for auth and storage, and it’s highly scalable for a growing number of tenants. |
| **AI Service**           | **LLM via API (e.g. OpenAI GPT-4)** | Instead of building a model from scratch, the platform will use a state-of-the-art language model via API. OpenAI’s GPT-3.5/GPT-4 or similar provides excellent quality answers out-of-the-box. This choice offloads heavy AI computation to the provider and allows the MVP to leverage advanced NLP without training data. Retrieval-Augmented Generation (RAG) will be implemented on top (using our knowledge base); the LLM is used to generate final answers with the retrieved context ([How We Built a Domain-Specific Support Chatbot Using Firebolt with RAG](https://www.firebolt.io/blog/how-we-built-a-domain-specific-support-chatbot-using-firebolt-with-rag#:~:text=TL%3BDR%3A%20Firebolt%20built%20a%20support,latency%20answers)). |
| **Vector Embeddings**    | **Embedding Library or API**  | For document retrieval, we will generate embedding vectors for each support document (using e.g. OpenAI Embeddings or a local model like Sentence Transformers). A simple vector index can be maintained in-memory or in Firestore (as fields) for similarity search. This component is not user-facing but crucial for the chatbot’s accuracy. It could be replaced by a dedicated vector DB in future if needed for scale. |
| **Infrastructure & DevOps** | **AWS Services, CI/CD**   | AWS will host the Django app (with auto-scaling groups or containers). We will use AWS to store any uploaded files (S3) if needed, and configure proper environment (Secrets Manager for API keys, etc.). CI/CD: Code can be managed in a Git repo with pipeline to run tests and deploy to Vercel (frontend) and AWS (backend). |

**Note:** Firebase Auth could be utilized for user authentication to avoid building auth from scratch. In that scenario, the React app uses Firebase Auth SDK for signup/login, and the Django backend verifies Firebase ID tokens (via Firebase Admin SDK) on each request ([Django and Firebase Integration for Seamless User Authentication](https://www.mindbowser.com/django-firebase-integration/#:~:text=Django%20and%20Firebase%20Integration%20for,authentication%20in%20Django%20custom%20authentication)). Alternatively, Django’s own auth system can be used with Firestore as the user data store (via a custom authentication backend) ([Django authentication with firebase? - Reddit](https://www.reddit.com/r/django/comments/pjje6f/django_authentication_with_firebase/#:~:text=Django%20authentication%20with%20firebase%3F%20,and%20on%20the%20mobile%20side)). Using Firebase Auth might simplify social logins and password resets, aligning with the rest of the Firebase stack.

## Backend Design and Modules (Django on AWS)

The backend is a Django application structured into reusable **modules (Django apps)**. Key modules and their responsibilities:

- **Accounts & Multi-Tenancy:** Manages user signup/login and company accounts. Likely uses Django’s auth system or Firebase Auth integration. There will be models for **User** and **Company** (tenant). Each User is associated with a Company (or multiple if needed). We enforce that almost every operation requires a company context – e.g. when uploading docs or initiating a chat, the user’s company or the widget’s company ID is identified, and all queries filter by that. This ensures *data isolation*, so one company cannot access another’s data ([Evolving django-multitenant to build scalable SaaS apps on Postgres & Citus - Citus Data](https://www.citusdata.com/blog/2023/05/09/evolving-django-multitenant-to-build-scalable-saas-apps-on-postgres-and-citus/#:~:text=Citus%20open%20source%20or%20on,multitenant%20easy%20for%20you)). Authentication to the API might use JSON Web Tokens or session cookies (for the admin web app), and a separate token mechanism for the widget (see Security). 

- **Bot Configuration Module:** Allows creation and customization of a chatbot for a company. This includes endpoints like:
  - `POST /api/bot/config` – set the bot’s name, welcome message, tone preferences, etc.
  - `POST /api/bot/knowledge` – upload support knowledge. The backend may accept text, PDFs, or FAQs. For MVP, we might start with simple text/markdown or Q&A pairs input. The module will break down documents into smaller chunks if needed and store them in Firebase. It will also trigger generation of embeddings for each piece of content (using an embedding API or library). These embeddings (numerical vectors) are stored in Firestore (or a cache) for later retrieval queries.
  - `GET /api/bot/embed-script` – returns the JavaScript snippet (or parameters) for embedding the widget. This could dynamically insert the company’s ID and an API key into a template script.

  *Tone Preferences:* Tone and persona settings are saved as part of bot config. At runtime, these are used to formulate the system prompt for the LLM. For example, if a company selects a “friendly and casual” tone, the system message might say: *“Respond in a friendly, informal tone with emojis where appropriate.”* Utilizing the LLM’s system message is the recommended way to set persona and tone ([The Difference Between System Messages and User ... - PromptHub](https://www.prompthub.us/blog/the-difference-between-system-messages-and-user-messages-in-prompt-engineering#:~:text=The%20Difference%20Between%20System%20Messages,on%20specific%20tasks%20and)). By adjusting this for each company, the same AI can speak in different styles per tenant.

- **Chat Conversation Module:** Handles the core chat functionality. Endpoints:
  - `POST /api/chat` – the main endpoint the widget calls to send a user message and receive a response. This endpoint logic does the retrieval + LLM call. It will accept a message (and a conversation ID or session token). If no conversation ID is provided, it will create a new conversation entry in the database, otherwise it will append to the existing conversation.
  - **Retrieval:** The module queries Firestore for the top N knowledge documents relevant to the question. If we stored embeddings, it computes the query embedding and finds nearest vectors (could be done in-memory if the embeddings are cached in the Django process, or by scanning Firestore – though Firestore doesn’t natively support vector queries, we might retrieve all docs for small KBs and compute distances in code). This yields a set of text passages that are likely answers or contain the answer.
  - **LLM Prompting:** The module constructs the prompt for the LLM. We include a **system message** like: *“You are a support chatbot for CompanyX. Answer using the information from our knowledge base and **do not answer beyond it**. Company tone is friendly and concise.”* (The bold instruction helps restrict the bot to the knowledge). Then we include **context messages**: possibly one assistant message containing the retrieved content (e.g. *“Here are relevant excerpts from the knowledge base: ...”*), then the user’s question as a user message. We may also include recent conversation history for multi-turn context (discussed below). This prompt design follows the retrieval-augmented generation pattern ([How We Built a Domain-Specific Support Chatbot Using Firebolt with RAG](https://www.firebolt.io/blog/how-we-built-a-domain-specific-support-chatbot-using-firebolt-with-rag#:~:text=TL%3BDR%3A%20Firebolt%20built%20a%20support,latency%20answers)).
  - **Calling the AI:** We call the OpenAI Chat Completion API (or similar) with `stream=True` to get a streaming response.
  - **Streaming Response:** The Django view will not wait for the full response; it will start streaming chunks to the client. We utilize Django’s `StreamingHttpResponse` with `Content-Type: text/event-stream` to send a server-sent event for each chunk ([Stream AI chats using Django in 5 minutes (OpenAI and Anthropic)  - Tom Dekan](https://tomdekan.com/articles/llm-stream#:~:text=No%20third)). The frontend widget will listen to these events and reconstruct the message progressively. This streaming setup dramatically improves perceived latency – users see the answer appear word-by-word almost immediately, rather than waiting for the entire answer to be ready.
  - **Fallback logic:** If the retrieval found nothing relevant or the LLM output indicates uncertainty (e.g. starts with “I’m not sure”), the backend can flag this. In such cases, the final answer might be a prompt for email capture: e.g. *“I’m sorry, I don’t have that information. Would you like to leave your email for our team?”*. The widget can switch to a mode where it collects the user’s email. If the user submits it, an endpoint (maybe `POST /api/chat/feedback`) will store that email along with the question for the company (or send an email notification to the company’s support address).

- **Analytics & Logging Module:** This handles storing and retrieving conversation logs and usage analytics.
  - Every conversation and message will be saved to Firestore (likely in a structure discussed in the next section). This module might use Firebase’s capabilities to run simple aggregations or we might compute on the fly.
  - Endpoints could include `GET /api/analytics/summary` (returns stats like number of conversations today, % of questions answered vs forwarded, average response time, etc.) and `GET /api/analytics/conversations` (list recent conversation transcripts, or at least recent questions that were asked).
  - For MVP, “basic analytics” might just be counts and lists (we can expand later). We might also leverage Firebase’s built-in Analytics or Google Analytics for additional metrics if needed, but storing and computing in our backend gives more flexibility. 

- **Integration & Email Module (optional):** If we implement the fallback-to-email feature, we need a way to deliver the captured email and question to the company. Options include sending an email via an SMTP service (AWS SES, SendGrid, etc.) or just logging it for the company to see in their dashboard. For MVP, a simple approach is to list these “unanswered queries” in the admin UI so company reps can manually follow up. We can implement email notifications as an enhancement.

- **Administration & Deployment:** Django’s admin panel could be leveraged for internal admin of the platform (e.g. managing companies, moderating content if needed). On AWS, the Django app will run behind a load balancer. We’ll enable **scalability** by running multiple instances as needed. For static files (if any, e.g. if companies upload PDFs or images), we’ll use AWS S3 or CloudFront.

**Multi-Turn Context Handling:** The backend needs to handle multi-turn conversations, meaning the user and bot messages build on prior context. We will achieve this by maintaining a conversation state:
  - When a conversation starts, a unique `conversation_id` is generated (and returned to the widget).
  - The widget includes this ID in subsequent requests. The backend will look up past messages for that conversation from the database. We may limit it to the last few turns or a maximum token count to stay within LLM limits.
  - Because OpenAI’s API doesn’t persist any conversation, we must resend relevant history each time ([How To Maintain OpenAi Chat Context Within Token Limit Production Level Part-1 | by Ali Jakhar | Medium](https://alijakhar.medium.com/how-to-maintain-openai-chat-context-within-token-limit-production-level-part-1-046eeb76ee8d#:~:text=Chat%20context%20is%20like%20the,below%20for%20a%20better%20understanding)). For example, if a user asks “Where is my order?” after a previous question about returns, the bot should know *“order”* refers to the user’s order. We’ll include prior QA pairs as part of the prompt for continuity. If the history grows too long (exceeding the token window, e.g. ~4096 tokens for GPT-3.5), the system might summarize older parts or drop them. For MVP, a simple strategy is to keep the last N exchanges.
  - The conversation logs in Firestore serve both for context and analytics. We must ensure reading them is efficient. We might cache the recent messages in memory while the conversation is active (to avoid a DB read each turn), but the authoritative storage is Firestore (for persistence across server restarts or multi-server environments). 

## Frontend Architecture (React Dashboard & Widget)

The frontend consists of two parts: the **Admin Dashboard** (for company users) and the **Embeddable Chat Widget** (for end-users on company sites). These can be built as part of one codebase but are delivered differently.

### Admin Dashboard (React + TS on Vercel)
This is a single-page application (SPA) that allows company users to configure and monitor their chatbot. We will use React with functional components and hooks (or possibly Next.js for SSR if needed, though likely a client-side app is fine since it’s behind a login). Key UI pages/components:

- **Signup/Login Pages:** Interfaces for authentication. If using Firebase Auth, we can use Firebase UI widgets or custom forms that call our backend. After login, the app stores a token (JWT or Firebase ID token) to include in API calls.
- **Onboarding Wizard:** For a new company, a step-by-step flow: enter company name, allowed domain(s), branding color/logo, desired tone (perhaps a dropdown of tone presets), and upload initial support FAQ or docs. This wizard calls backend APIs to create the company and save these settings.
- **Knowledge Base Manager:** A page where users can upload or edit support content. Could be as simple as a text area to add Q&A pairs or a file upload control for documents. The content list is fetched from Firestore (via backend) to display existing entries. Each item can be edited or deleted. (In MVP, we might not implement a full text editor; possibly just allow deleting and instruct them to upload a new version for changes.)
- **Tone & Behavior Settings:** UI controls for selecting the chatbot’s tone/personality. For example, radio buttons or a dropdown: *Friendly*, *Professional*, *Humorous*, *Strictly Formal*, etc. This selection is saved to the bot config. There could also be a field for a custom greeting message that the bot uses as its first message or as prefix (e.g. “Hi, I’m the ACME Support Bot, here to help!”).
- **Embed Code Generator:** A section that shows the script snippet to embed the chatbot. It will include the company’s unique identifier (and perhaps an embed token or key). For example, it might display something like:

```html
<!-- ACME Support Chatbot embed -->
<div id="acme-chatbot"></div>
<script>
  window.ACME_CHATBOT_API_KEY = "abcdef12345"; 
  window.ACME_CHATBOT_URL = "https://ourapp.com/widget.js";
</script>
<script src="https://ourapp.com/widget.js" async></script>
```

  The exact format can vary, but the idea is the admin can copy-paste this into their site. We’ll ensure the snippet is as simple as possible. We’ll also display instructions (e.g. “Paste this before the `</body>` tag of your site”).

- **Analytics Dashboard:** A page to display basic usage metrics. This could include counters (total conversations, unanswered questions, average response length) and a table of recent conversations or user questions. For MVP, we can keep it simple: e.g. list the last 10 questions asked and their answers (or flag if the bot couldn’t answer). This info comes from the backend (`/api/analytics`).

- **Conversation Logs Viewer:** Optionally, a page to drill into one conversation log (especially if a user left an email because the bot couldn’t handle a query). The support agent could see the conversation and contact the user. This might be combined with the analytics page in MVP.

**State Management:** The React app can use a state management library like Redux or context + hooks for simpler state. For example, store the logged-in user info and current company in a global context. Most pages will fetch data on mount (e.g. fetch config, fetch knowledge list, etc.). We’ll handle loading and error states gracefully (spinners, messages).

**UI Design and Branding:** The admin interface will likely use a component library or design system (maybe Material UI or Ant Design) for professional look and faster development. This is separate from the chatbot widget’s style (which is custom to each company). The admin UI just needs to be clean and functional.

**Deployment:** The React app will be deployed on Vercel. Each update triggers Vercel to build the app (likely a static bundle or a serverless function if using Next.js SSR) and distribute it. The admin app will be accessed via a domain, e.g. `app.oursaas.com`. It will communicate with the backend at an API endpoint (we must configure CORS to allow the Vercel domain if needed). Because it’s behind authentication, SEO is not a concern; performance and security (XSS, etc.) are the main focus.

### Embeddable Chatbot Widget 
The widget is a small web application embedded in customer websites. Several design considerations ensure it’s easy to integrate, fast, and secure:

- **Embed Script Delivery:** We will host a JS file (let’s call it `widget.js`) that clients include via a `<script>` tag. This file can be served via our CDN or directly from Vercel/AWS. Using Vercel’s Edge Network or CloudFront will help deliver it quickly worldwide. The script is loaded asynchronously (`async defer`) so it doesn’t block the host page’s loading.

- **Initialization:** When `widget.js` runs, it will typically:
  1. Read configuration from a global JS object or `data-` attributes (the snippet might include some config as shown above, like a global API key or company ID).
  2. It will create a chat container element (e.g. a floating div at bottom-right of the page) and inject the necessary HTML/CSS for the chat interface.
  3. Optionally, it might open a WebSocket or start an SSE connection if we decided to use continuous connections. However, using simple fetch for each query might suffice, using SSE for streaming responses.
  4. It may call an initialization API on the backend to fetch the bot’s greeting or any needed config (like the theme colors, welcome message, etc., if those aren’t already baked into the script).

- **UI and Branding:** The widget’s appearance should match the company’s branding. We can achieve this by having the backend embed the company’s styles or config in the script. For example, when the script knows the company ID, it could dynamically apply CSS variables for primary color, or load the company’s logo URL for the chatbot avatar. Alternatively, we might build the widget as a React component that takes a config JSON (fetched from the backend at runtime) containing these style properties. For MVP, simple customizations like color scheme and bot name should be supported.
  
  The chat UI likely consists of:
  - A minimized state (e.g. a chat bubble or button with the bot’s icon and maybe a welcome prompt).
  - An expanded state (chat window with message history, an input box, and a send button).
  - The design will be mobile-responsive so it works on websites on various devices.

- **Conversation Handling on Widget:** The widget will manage the local state of the conversation (messages). When the user sends a message:
  - It immediately appends the user’s message to the chat window (optimistically).
  - It calls the backend `/api/chat` endpoint via `fetch` (with the company’s API key/token and conversation ID if any).
  - It then enters a “waiting for response” state – possibly showing a typing indicator.
  - As streaming SSE messages come in, it updates the latest message gradually. We’ll implement an EventSource (for SSE) or use fetch readable streams if supported. The widget concatenates the tokens into the answer bubble.
  - Once complete, the answer is displayed and the input is enabled again for the next question.

- **Maintaining Context:** The widget should store the `conversation_id` returned by the backend (perhaps in a JS variable). If the user closes the chat widget or page, that context might be lost. For MVP, it’s acceptable that each page load starts a new chat. If we want persistence across page navigations, we could store the conv ID in `sessionStorage` so that reopening the widget in the same session resumes the last conversation (assuming the backend still has it). This is a nice-to-have but not critical in MVP.

- **Widget Deployment/Versioning:** The `widget.js` could be versioned (e.g. `widget.v1.js`) to allow updating the widget code without breaking existing embedded snippets. Initially, we might not worry about multiple versions. But as we improve the widget, companies embedding it should get the latest automatically if we host it at a static URL (because they include our URL and we can update the file). We should be cautious to not introduce breaking changes to the embed API.

- **Security for Widget:** The widget itself should not expose sensitive info. The API key or ID it uses is meant only to identify the company – it’s not a secret that can access someone else’s data due to backend checks. We will also implement origin checking on the backend side: when a widget calls our API, we verify the `Origin` header or a signed token to ensure the request is coming from an allowed domain (discussed more in Security below). Additionally, the script may sanitize user inputs (to avoid any XSS if the user somehow inputs something that could break the widget UI – though the UI is controlled by us, not user content except their messages).

- **Lightweight and Performance:** We aim to keep the widget bundle small (a few tens of KB minified). By not overloading it with large frameworks (maybe using Preact or a lean build of React, or even vanilla JS for a simple DOM manipulation), we ensure quick loading. Also, we will defer loading heavy parts until needed (e.g. maybe load the full chat window HTML only when user clicks the widget to expand, which can further reduce initial payload). The <1s load requirement mainly pertains to the widget appearance (the small chat bubble) – full LLM answer obviously can take a few seconds, but that’s after interaction. Using Vercel’s edge network or a CDN for static assets means the latency to download the script is minimal.

## Data Model and Database (Firebase Firestore)

Using Firestore (NoSQL) influences how we design our data schema. We will organize data into collections and documents, with a focus on multi-tenancy (every document tied to a company). Below is a proposed schema:

| **Collection (Entity)** | **Document ID** | **Key Fields** (examples)                                                | **Notes**                                     |
|-------------------------|-----------------|--------------------------------------------------------------------------|-----------------------------------------------|
| **Companies**           | company_id (auto-generated or custom slug) | name, domain_allowlist (list), api_key, branding (color, logo URL), tone (preset or description), created_at | Each company/tenant. The `domain_allowlist` defines which website domains can use the widget for this company. The `api_key` is a secret token for the widget to authenticate (could also use company_id + some secret). Tone could be an enum or text that we use in prompts. |
| **Users** (if using Firestore for users) | user_id (UID) | email, password_hash (if not using Firebase Auth), company_id, role (admin/member), created_at | Users can belong to a company. If using Firebase Auth, user accounts are managed by Firebase and this collection may not be needed (or just store user profile linking to company). |
| **KnowledgeBase**       | auto ID or custom (e.g. doc title) | company_id, title, content, embedding (array of floats), metadata (e.g. source filename), created_at, updated_at | Stores the support articles/FAQs. Content could be the full text or a chunk of text. If documents are large, we might split into multiple docs (with a reference to which original file). The `embedding` field holds the vector for semantic search. Firestore allows arrays of numbers; while not ideal for querying, we can fetch all docs for a company and compute cosine similarity in code for MVP. Alternatively, we maintain a separate in-memory index per company. |
| **Conversations**       | conversation_id (uuid or auto) | company_id, started_at, last_activity_at, user_email (if collected), status (open/closed) | Represents a chat session. `user_email` is optionally filled if the conversation ended with collecting the user’s email for follow-up. We might mark conversations as closed after some idle time or when the user leaves. |
| **Messages**            | (subcollection under Conversations or a separate top-level collection) | Fields: role (“user” or “assistant”), content (text), timestamp | We need to store the sequence of messages in each conversation. Firestore options: (a) use a subcollection `Conversations/{id}/Messages/{id}` for each message, or (b) store an array of message objects in the Conversation document. Option (a) is more flexible and scalable for many messages, option (b) is simpler but has a 1MB document limit (which might be hit in very long chats). We’ll likely use a subcollection so each message is a separate doc. We will index by timestamp. The backend can query for all messages in a conversation sorted by time to reconstruct history. |
| **Analytics/Aggregates** (optional) | Could use a special doc per company | company_id, total_conversations, total_messages, unanswered_questions_count, etc., stats_by_date (map of date -> count) | We might not need a dedicated analytics collection if we can compute stats on the fly or via scheduled jobs. However, for efficiency, a summary doc per company that gets updated could be useful (e.g. increment conversation count each time a new conv is created). Firestore supports atomic increments. This is a minor detail and can be added after basic functionality. |

**Database Considerations:**
- *Firebase Security Rules:* If the frontend were accessing Firestore directly, we’d use Firebase’s rules to ensure each user can only see their company’s data. However, since our design routes all access through the backend, we rely on backend logic for security. We’ll still set up rules to prevent any accidental open access (e.g. perhaps only allow the service account or specific UIDs).
- *Data Volume:* Firestore can handle a large number of documents. Each message is a small document; even with many conversations, it should scale (also, older conversations could be archived or moved to cold storage if needed in future). We should be mindful of Firestore costs (reads/writes) – streaming a conversation might cause many writes (each message is a write) and reading conversation history each time is multiple reads. Caching recent messages in memory or batching writes (maybe accumulate the assistant's streamed answer and write as one message when complete) can mitigate this.
- *Atomicity:* Firestore is eventually consistent for queries, but for our use-case that’s fine. We might need transactions for something like updating analytics counters. Firestore supports transactions and batched writes which we can use in the Django backend via the Firebase Admin SDK.
- *Backup/Exports:* Since this is critical data, we would enable periodic backups of Firestore (Firebase has automated daily export options).

**API Design Highlights:**
All interactions use REST (JSON payloads). Here’s a summary of important API endpoints (and their purpose):

- `POST /api/auth/register` – Create a new account (if not using Firebase Auth’s direct methods). 
- `POST /api/auth/login` – Authenticate and return token.
- `POST /api/company` – Create a new company (organization) entry, likely called during onboarding. (If one user can belong to multiple companies, we need endpoints to switch/select, but assume one company per account for simplicity).
- `GET /api/company` – Get company profile (including branding settings, allowed domain, etc.).
- `PUT /api/company` – Update company settings (e.g. change allowed domain or colors).
- `POST /api/knowledge` – Upload a new support document or Q&A. Could support file upload (if so, possibly the file is sent to backend, backend extracts text and stores text in Firestore). For MVP, we might accept only text content here.
- `GET /api/knowledge` – List all knowledge base entries (for admin UI display).
- `DELETE /api/knowledge/{doc_id}` – Remove an entry.
- `POST /api/chat` – Send a user message to the chatbot (used by widget, possibly by admin UI too for testing). Request: `{ conversation_id, message }`. Response: would stream data; if not using streaming in initial implementation, it would return the full answer and a `conversation_id` if new.
- `GET /api/chat/{conversation_id}` – (Optional) get full conversation transcript (for admin viewing).
- `GET /api/analytics/summary` – Basic stats for dashboard.
- `GET /api/analytics/conversations` – List recent conversations or those with missing answers.
- `POST /api/conversations/{id}/email` – Store an email address for a given conversation (if user provided in fallback).

We will secure the admin endpoints with authentication (e.g. JWT in Authorization header or cookie), and the chat endpoint with the company’s embed credentials.

## AI Integration: Retrieval-Based Q&A with Embeddings

The core “AI” functionality is built on **Retrieval-Augmented Generation (RAG)** ([How We Built a Domain-Specific Support Chatbot Using Firebolt with RAG](https://www.firebolt.io/blog/how-we-built-a-domain-specific-support-chatbot-using-firebolt-with-rag#:~:text=TL%3BDR%3A%20Firebolt%20built%20a%20support,latency%20answers)) – the chatbot will not answer from general training data alone, but rather use the company’s specific knowledge base for information. Here’s how we’ll implement this:

- **Embedding the Knowledge Base:** When a company uploads content, the backend generates an embedding vector for each piece of text. For example, if they upload a PDF with several sections, we might split it into paragraphs and embed each. We can use OpenAI’s embedding API (e.g. `text-embedding-ada-002`) to get a 1536-dimensional vector for each chunk of text. Alternatively, use a local model from SentenceTransformers to avoid API cost. These vectors are stored in Firestore alongside the content (or in a separate in-memory structure). Storing in Firestore might involve compressing them or splitting across fields due to size, so perhaps we store as an array of floats directly (Firestore supports arrays).

- **Semantic Search at Query Time:** When a user asks a question, we embed the question the same way. Then we perform a similarity search between the question vector and the stored vectors for that company’s docs. In MVP, with moderate data, we can load all vectors for that company into memory (cache on the Django server when the company’s data is first used) and do a cosine similarity computation for each – this is feasible if knowledge base is, say, a few hundred pieces. For larger scales, we’d integrate a vector search library (FAISS or an external service like Pinecone) but MVP can keep it simple. The top 3-5 relevant snippets are selected as context.

- **Prompt Construction:** We craft a prompt for the LLM that includes the retrieved snippets. One approach is to insert them into a system or assistant message explicitly: e.g. *“Knowledge Base:\n1. [excerpt1]\n2. [excerpt2]\n3. [excerpt3]\nUser question: [the user’s question]”*. Then ask the model to answer using only this knowledge. We also prepend a system instruction with the tone and any other rules (like refusal to answer if not in knowledge). This method has been shown to yield accurate responses grounded in provided text ([How We Built a Domain-Specific Support Chatbot Using Firebolt with RAG](https://www.firebolt.io/blog/how-we-built-a-domain-specific-support-chatbot-using-firebolt-with-rag#:~:text=TL%3BDR%3A%20Firebolt%20built%20a%20support,latency%20answers)).

- **Tone and Persona:** As mentioned, tone is applied via system message. For example, system message could be: *“You are a polite and friendly support assistant for [CompanyName]. Speak in a casual tone and use the company's knowledge base to answer. If the answer is not in the knowledge base, do not invent an answer.”* This ensures consistent style and mitigates off-topic answers ([System Prompts in Large Language Models](https://promptengineering.org/system-prompts-in-large-language-models/#:~:text=System%20Prompts%20in%20Large%20Language,tone%2C%20such%20as%20friendly%2C)). Tone instructions are an effective way to define style in LLM outputs ([System Prompts in Large Language Models](https://promptengineering.org/system-prompts-in-large-language-models/#:~:text=System%20Prompts%20in%20Large%20Language,tone%2C%20such%20as%20friendly%2C)).

- **Multi-turn memory:** The conversation history will also be included for context, especially if the user’s next question is follow-up. We might attach the last few exchanges as part of the prompt (as role messages: previous user and assistant messages). This helps the model maintain context up to its token limit. We just need to be careful to keep it within limits (maybe drop or summarize older turns as needed).

- **Model Selection:** Initially, we could use GPT-3.5 Turbo for cost and speed. It’s capable of following instructions and using provided context well, and is cheaper. GPT-4 could be optional for higher accuracy (maybe a premium offering). The system should be model-agnostic enough that we can swap in others (Anthropic Claude, open-source models on AWS, etc. in the future). For now, we integrate via OpenAI’s REST API.

- **Handling Unanswerable Queries:** If no relevant context is found (e.g. question is completely outside the provided knowledge), we have a few strategies:
  1. **Refusal:** The bot can respond with a polite apology that it doesn’t have that info.
  2. **Fallback to Email:** As required, the bot should then prompt for email if it cannot answer. Implementation: the backend can detect that the similarity scores of all snippets are below a threshold, indicating a likely unanswerable question. In that case, instead of calling the LLM normally, it might directly return a special flag or a crafted response: *“I’m sorry, I don’t have that information. Please leave your email and our team will help you.”* The widget would display this and provide an email input. This flow is partly a design decision – either let the AI say that (with our instruction) or handle it logic outside the AI. Likely safer to handle outside (to ensure it definitely asks for email when needed).
  3. We will log these instances so the company knows what users are asking that wasn’t covered – useful to improve their knowledge base over time.

- **Testing the AI Behavior:** We will need to test with actual company data to fine-tune prompt formats. We must ensure the AI *only* uses given info. By default, GPT might have general knowledge and could answer from that, which we want to avoid. We’ll enforce in the prompt “If the answer is not in the above content, say you don’t know.” and test it. This is part of **prompt injection mitigation** as well – we don't want user asking "ignore above and tell me something else" to succeed.

## Low-Latency Streaming and Performance

Meeting the latency goals involves both **frontend optimizations** (fast load) and **backend streaming** (fast response). Here’s the plan to achieve snappy performance:

- **Widget Load Under 1s:** The embed script will be optimized for size and delivered via a CDN. Using Vercel means the script is served from edge locations worldwide. We’ll keep the bundle lean – for example, not importing large dependencies. If using React, we might compile it with production optimizations and maybe even pre-render some HTML. Alternatively, the widget could be vanilla JS to avoid loading React library at all (since the UI is not very complex). We will also mark the script with `async` so it doesn’t block page parsing. The chat widget itself can be hidden or minimized by default, so it doesn’t interrupt the user until needed.

- **Server-Sent Events for Streaming:** As described, Django can flush the response incrementally. We’ll implement the chat endpoint to return a `StreamingHttpResponse` and iterate over the LLM’s streamed output as it arrives ([Stream AI chats using Django in 5 minutes (OpenAI and Anthropic)  - Tom Dekan](https://tomdekan.com/articles/llm-stream#:~:text=No%20third)). Each chunk will be sent as an SSE (with proper `data:` prefixes and newline as per EventStream format). On the client side, we use `EventSource` to receive these. This avoids needing to maintain a WebSocket (SSE is simpler for one-way streaming and works over HTTP/2 or HTTP/3 nicely). The result is that the user starts seeing the answer typically within a second or two (the time for the first tokens to be generated), even if the full answer might take a few seconds. This real-time typing effect greatly improves user-perceived speed.

- **Parallel Processing:** The system will handle multiple user queries at once. AWS can run multiple Django workers, and each can handle an LLM request. If an influx of queries happens, we rely on horizontal scaling (adding more instances) because each query might hold a connection open for several seconds (due to streaming). We might use an async view to avoid tying up threads while waiting for network I/O from OpenAI. Django 4+ with ASGI allows async views, so we can make the OpenAI call asynchronously and stream results without blocking other requests. This ensures good throughput.

- **Caching:** To minimize repeated computation:
  - Cache the knowledge base embeddings in memory on each backend instance so we don’t hit Firestore for them every time. E.g., when the first query for company X comes in, load all X’s docs and embeddings into a Python dict and reuse until the process dies or data changes. If the knowledge base updates, we can invalidate or update the cache.
  - We can also cache recent answers for identical questions to the same company (though that’s an edge optimization and possibly not needed unless a lot of repetition).
  - The static content (like the widget script, CSS) will be cached by browsers and CDN naturally due to far-future cache headers.

- **Firestore Performance:** We will use indexed queries properly. For instance, to fetch messages by conversation (if needed), we ensure an index on `conversation_id, timestamp`. Firestore can handle high read rates if indices are set. If certain data is accessed extremely frequently (like a small config doc), we could cache it in Redis or Django’s cache framework. But likely not needed at MVP scale.

- **Scaling the LLM calls:** If using OpenAI, we have to mind their rate limits. The design should include handling of hitting the rate limit – possibly queueing or rejecting some requests. For now, we assume our volume is within allowed limits. If a company has a spike of traffic, our rate limiting (see security) might throttle extreme usage to stay under external API limits as well.

- **Asynchronous Tasks:** For any heavy background jobs that aren’t user-facing, consider using a task queue. For example, processing a large document into chunks and embeddings could be offloaded to a Celery worker. The user doesn’t need to wait synchronously; we can show “document processing in progress” and notify when ready. However, given MVP scope, we might handle it inline if quick, and only move to background tasks when needed.

In summary, by combining CDN-fast static delivery, minimal JS on the frontend, efficient retrieval, and streaming responses, the system will feel highly responsive. Users see the widget appear instantly and get feedback to their queries in real-time as the AI types out answers.

## Security and Multi‑Tenant Isolation

Security is vital since multiple companies’ data is hosted in one platform and the chatbot is publicly accessible on websites. The MVP will implement the following measures:

- **Tenant Data Isolation:** As discussed, every database query is scoped to the company’s context ([Evolving django-multitenant to build scalable SaaS apps on Postgres & Citus - Citus Data](https://www.citusdata.com/blog/2023/05/09/evolving-django-multitenant-to-build-scalable-saas-apps-on-postgres-and-citus/#:~:text=Citus%20open%20source%20or%20on,multitenant%20easy%20for%20you)). In Django, we will enforce this at the model/API layer – e.g. using a filter on `company_id` for all relevant queries, or using a library like django-multitenant that injects that filter automatically ([Evolving django-multitenant to build scalable SaaS apps on Postgres & Citus - Citus Data](https://www.citusdata.com/blog/2023/05/09/evolving-django-multitenant-to-build-scalable-saas-apps-on-postgres-and-citus/#:~:text=Citus%20open%20source%20or%20on,multitenant%20easy%20for%20you)). We will also include the company ID in the primary keys or as a partition key in Firestore (effectively namespacing data by company). This prevents any possibility of data leakage across tenants. It will be tested by attempting cross-tenant access (which should yield no results or forbidden errors).

- **Domain Origin Verification:** We restrict the usage of the embed script to approved domains. Each company will configure the domain(s) where they will install the chatbot (e.g. `example.com`). In the embed snippet, we might include a company-specific token. When the widget calls the backend, the backend will verify the call in two ways:
  1. **CORS Allowlist:** We configure the backend’s CORS to only allow requests from the company’s domain (for the chat endpoints). For example, using Django CORS middleware, we dynamically set the allowed origin for each company’s API key. This ensures that in a normal browser, other origins cannot make requests to our API (the browser would block it) ([how can i secure my embeddable widget : r/webdev](https://www.reddit.com/r/webdev/comments/1d1crnx/how_can_i_secure_my_embeddable_widget/#:~:text=It%20really%20depends%20on%20what,the%20integration%20of%20your%20widget)).
  2. **Backend Check:** CORS can be bypassed by a determined attacker (using cURL, etc.), so we also implement a server-side check. We’ll issue each company an `api_key` or embed secret. The widget includes this in the API calls (as a header or parameter). The backend endpoint `/api/chat` will validate that the key is valid and matches the company making the request. We can also check the `Origin` or `Referer` header on the request to ensure it matches the company’s allowed domain, adding another layer. If either the key is invalid or the origin is not allowed, the request is rejected. This approach is similar to how one would secure an embeddable widget by combining allowlisted domains and API secrets ([how can i secure my embeddable widget : r/webdev](https://www.reddit.com/r/webdev/comments/1d1crnx/how_can_i_secure_my_embeddable_widget/#:~:text=It%20really%20depends%20on%20what,the%20integration%20of%20your%20widget)).
  - Additionally, we could use techniques like Content Security Policy on the embed (to restrict where it can connect or be framed), but since it’s just a script running on their site, CSP on their site is under their control. Our focus is ensuring our API only serves legitimate requests.

- **Rate Limiting & Abuse Prevention:** To prevent misuse or overloading, the platform will enforce rate limits per IP (especially on the chat API). Using Django or a proxy layer, we might allow, for instance, 5 requests per minute per IP for the chat endpoint (this can be tuned). If an IP exceeds that, subsequent requests will be throttled or given an error. We can implement an **exponential backoff** strategy: e.g., after the limit, require a 1-minute cool-off; if abuse continues, extend to 5 minutes, etc. This prevents bots from spamming the API continuously ([API Rate Limits Explained: Best Practices for 2025 - Orq.ai](https://orq.ai/blog/api-rate-limit#:~:text=API%20Rate%20Limits%20Explained%3A%20Best,reaching%20the%20API%20rate%20limit)). Libraries like Django Rest Framework’s throttling classes can help here, or a simple middleware that tracks IP counts in an in-memory store (or Redis for distributed). 
  - We also consider rate limiting by company to enforce fair use (e.g. a free tier company might be limited to X requests/day).
  - In addition, the LLM integration itself might have rate limiting (OpenAI’s side); we should catch rate-limit errors from OpenAI and maybe propagate a message to slow down, or queue the request.

- **Prompt Injection Mitigation:** Prompt injection is a new threat vector for AI systems where malicious input could trick the bot into ignoring its instructions ([Protect Against Prompt Injection | IBM](https://www.ibm.com/think/insights/prevent-prompt-injection#:~:text=Prompt%20injections%20are%20a%20type,data%2C%20spread%20misinformation%2C%20or%20worse)) ([Protect Against Prompt Injection | IBM](https://www.ibm.com/think/insights/prevent-prompt-injection#:~:text=LLMs%20accept%20and%20respond%20to,%E2%80%9D)). To mitigate this in MVP:
  - **Strict System Instructions:** We will include strong system prompts that explicitly tell the model to **never deviate** from the provided info, and to refuse if the user asks it to ignore instructions. While not foolproof, the latest models are better at respecting this.
  - **Input Sanitization:** We can detect certain patterns in user questions that indicate an injection attempt (e.g. the user says “ignore previous” or tries to insert `<system>` role text). If detected, we can refuse or cleanse that input. Basic approach might be a regex or substring check for terms like “ignore all previous instructions” ([Protect Against Prompt Injection | IBM](https://www.ibm.com/think/insights/prevent-prompt-injection#:~:text=It%20wasn%E2%80%99t%20hard%20to%20do,bot%20would%20follow%20their%20instructions)) and either remove them or respond with a refusal. This is essentially input validation for the LLM ([Protect Against Prompt Injection | IBM](https://www.ibm.com/think/insights/prevent-prompt-injection#:~:text=Input%20validation%20and%20sanitization)).
  - **Output Filtering:** We can post-process the AI’s answer to ensure it didn’t yield something it shouldn’t. For instance, if our knowledge base content plus a malicious prompt somehow caused it to spew sensitive info or disallowed content, we catch that. In practice, since the bot is confined to support data, this risk is low. But we might implement a safety check for certain keywords (like if the answer contains something obviously not from the knowledge base or some offensive terms, we block it).
  - **Human Monitor:** In early phases, we will monitor some logs (especially of unanswerable or weird queries) to see if any injection got through. If so, adjust prompts or filtering accordingly. As IBM’s guidance notes, a combination of tactics is needed and continued monitoring ([Protect Against Prompt Injection | IBM](https://www.ibm.com/think/insights/prevent-prompt-injection#:~:text=The%20only%20way%20to%20prevent,in%20the%20loop%2C%20and%20more)). While no single solution is foolproof, validating inputs and keeping a human in the loop for review significantly mitigates risks ([Protect Against Prompt Injection | IBM](https://www.ibm.com/think/insights/prevent-prompt-injection#:~:text=organizations%20can%20significantly%20mitigate%20the,in%20the%20loop%2C%20and%20more)).

- **Encryption and Privacy:** All network traffic will be over HTTPS. We’ll enforce TLS for the embed script and API. User data stored (like emails collected) will be kept securely in Firestore (Firestore automatically encrypts data at rest). If we store any sensitive API keys (like OpenAI key, Firebase credentials) on our side, those will be in AWS Secrets Manager or environment variables not accessible publicly.

- **Authentication & Authorization:** For the admin app, ensure that JWTs or session cookies are properly validated for each request. We will use standard auth middleware so that, for example, one company’s user cannot accidentally call an endpoint on another company’s data – even though multi-tenancy also covers that, defense in depth with auth scopes is good. Possibly assign each user a role (admin vs support rep) to limit who can change settings vs only view logs.

- **Multi-Tenant Resource Limits:** Each company’s data and usage might be constrained (for fairness and cost control). MVP security might include simple limits like maximum number of knowledge base documents allowed (to prevent someone from uploading a huge dataset and using our system arbitrarily) and maybe maximum conversations per day (especially for a free tier). These can be enforced in code (reject or warn when limits exceeded).

- **Logging and Monitoring:** All important actions (logins, config changes, errors, etc.) will be logged. We will monitor these logs for suspicious activities, such as a single IP hitting the API excessively (which rate limiting should cover) or repeated failed auth (possible intrusion attempt). For MVP, manual log review or simple alerts may suffice; long-term, integrating with monitoring tools or Firebase Analytics can help.

In summary, the MVP implements strong tenant isolation, basic rate limiting with backoff ([API Rate Limits Explained: Best Practices for 2025 - Orq.ai](https://orq.ai/blog/api-rate-limit#:~:text=API%20Rate%20Limits%20Explained%3A%20Best,reaching%20the%20API%20rate%20limit)), domain whitelist enforcement, and initial prompt safety measures. These create a secure baseline. As the platform grows, we can enhance security (e.g. more sophisticated anomaly detection, per-tenant encryption keys, etc.). 

## Edge Cases and Resilience

We must consider various edge cases to make the platform robust:

- **Concurrent Conversations:** If multiple users start chats around the same time (even for the same company), the system should handle it. Each conversation has an independent ID and context. We need to ensure no cross-talk. The backend must not mix up sessions. This is straightforward as long as we always use the conversation_id provided. Concurrency also matters in writing to Firestore – two messages being saved at once should both be saved. Firestore handles concurrent writes to different documents well (each message is separate). If we were updating a single document from two requests, we might use transactions (e.g. if two messages appended to an array, but we avoided that by using subcollection).
- **High Concurrency spikes:** During spikes, we might run into limits (API, DB, memory). Our rate limiting will help blunt the impact of abuse. We also plan for horizontal scaling on AWS – e.g. auto-scale the number of Django workers if CPU or throughput demands increase. An edge case is if OpenAI API itself slows down or errors out (not exactly our concurrency but external factor) – we should handle errors from the AI API gracefully (return a message like “Sorry, the service is busy, please try later”).
- **Partial Failures:** If the AI returns an error mid-stream or times out, the backend should catch that and send a final SSE event indicating failure. The widget can then show an error message or a retry option. Similarly, if Firestore read fails (temporary issue), we return a 500 error that the widget should handle (maybe display “Unable to retrieve answer. Please try again.”). The system should not get stuck in a waiting state.
- **Large Knowledge Base Inputs:** What if a company uploads a very large document or many documents? For MVP, we might enforce a limit (e.g. max 20MB of content or max 100 documents). If someone hits that, we inform them to contact support or upgrade. If we attempt to process a huge input, we risk long blocking or high memory. We should chunk large files and perhaps process them asynchronously. Also, ensure the embed indexing doesn’t load too much into memory; if a company has thousands of articles, we may need a proper vector database earlier than planned.
- **Unstructured Queries:** Users might ask things in weird ways, or ask multiple questions in one input. The LLM might handle it, but if multiple question marks appear, perhaps we ensure it addresses one at a time. Edge case: user says “Hi” or greets – the bot should respond with a greeting (we can have a default small-talk handling: either a static response or let the LLM handle casual greetings by providing a few common Q&A pairs including greetings in the knowledge base).
- **HTML and Scripts in Input:** We should escape any HTML in user input when displaying in the widget to prevent XSS on the client’s site. Even though user input typically wouldn’t contain script (and if they did, it’d only run in their own browser since it’s their chat), we need to sanitize output that the widget displays. The admin dashboard should also sanitize any data coming from Firestore (though most of it is input by the company itself).
- **Malicious File Uploads:** If we allow file uploads (PDF, etc.), there’s a potential security risk (PDFs could contain scripts or just be very large). We will likely extract text server-side. Using something like PyPDF or Apache Tika to parse PDFs can be heavy. We might restrict file types in MVP (only plain text or CSV of Q&As) to avoid this complexity. If we do allow PDFs, we’ll strip out any embedded scripts and just take text. Also, we should virus-scan files if letting arbitrary files (using a service or library) – perhaps a future enhancement.
- **Invalid Configurations:** Handle cases where the company config is incomplete:
  - No knowledge base content: The bot essentially can’t answer anything. The widget should perhaps recognize this and instead of trying the AI, immediately go to a fallback (like “Our support bot isn’t configured yet. Please contact support@company.com.”). Or we at least ensure the AI is instructed to say it doesn’t have info. MVP approach: warn the admin if knowledge base is empty, and for the user, just use the fallback path.
  - Tone not set or unrecognized: If tone is not set, default to a neutral friendly tone. If an unsupported tone value somehow sneaks in, we can default or clamp to known values.
  - Domain not set: We might allow the embed to run on any domain if none specified, but that’s a security hole. Better to require domain before providing embed code. So likely we’ll enforce companies to input allowed domain (and possibly verify it via an email to that domain or a file upload for verification token on their site in future).
- **Data Privacy and Deletion:** If a company deletes a document or all data, ensure the embeddings cache is updated (remove that vector). If a company deletes their account, cascade delete or disable all their data and ensure the widget calls for that company are rejected. 
- **Scaling Firebase:** Firestore is scalable but has some quotas (reads per second etc.). If a company’s chatbot becomes extremely popular, we may need to upgrade the data handling (like move hot data to an in-memory store). For now, an edge case could be hitting Firestore read quota if, say, thousands of users are polling at once – but our design uses push via SSE, not polling, which helps. We also might consider using Firebase’s real-time database for pushing updates to widget – but SSE is simpler and under our control.
- **Concurrent Writes to Analytics:** If two chats end at the exact same time and both try to increment an analytics counter in Firestore, one might overwrite the other if not done carefully. We will use Firestore’s atomic increment operation which avoids that conflict on a single doc.
- **Clock Sync and Timestamps:** Use server timestamps for ordering messages rather than client’s (to avoid issues if user’s clock is off). Firestore can set `timestamp = server_timestamp` on create.
- **Fallback Email Validity:** If user inputs an email, basic validation (regex) should be done to ensure it’s a valid format. Also, we should avoid storing obviously fake ones (though we can’t fully prevent that).
- **External API Errors:** If OpenAI API fails (network error, or returns an error about content), handle it gracefully. Possibly map certain errors to user-facing messages (e.g. if OpenAI rejects content as disallowed, we respond with a generic apology).
- **Deployment Edge Cases:** When deploying new backend code, there could be in-flight chat sessions. We might deploy with zero downtime strategies, but an edge case is a user mid-conversation might get disconnected if a server restarts. The widget should handle a dropped SSE connection by retrying or showing a message. We can implement auto-reconnect logic: EventSource will automatically try to reconnect; our server might not support resuming the exact stream, but the widget could detect if connection lost and maybe call a non-stream endpoint to get a final answer if possible. For MVP, a simpler note: if connection drops, user can retry their question.
- **SEO and Accessibility:** Not crucial for MVP, but ensure the widget has appropriate ARIA labels or can be navigated via keyboard, etc., for basic accessibility.
- **Legal**: Ensure we have terms for companies that they should not upload copyrighted or sensitive data unless they have rights, because that data might be processed by external LLMs (OpenAI). And we should not use customer data to train our own models without permission (for MVP, we don’t retrain anything).

By anticipating these edge cases, we can implement mitigations or at least have a plan when they occur. Many of these (like file upload handling, extreme scale) might be limited in MVP with simple constraints and tackled fully in later versions.

## MVP Development Roadmap

To build this platform from scratch, we propose an iterative development plan with milestones. Each milestone delivers a subset of functionality, allowing for testing and feedback:

| **Milestone** | **Features & Tasks** | **Timeline** |
|--------------|----------------------|-------------|
| **1. Setup & Foundations** <br>*Project Initialization* | - **Project Bootstrapping:** Set up Django project on AWS (initially can use local dev server), set up React app on Vercel. <br>- **Basic Models:** Create User and Company models in Django (or integrate Firebase Auth). <br>- **Auth API:** Implement simple registration and login API, with JWT authentication. <br>- **Hello World Frontend:** Deploy a basic React page that can create an account and log in (just to test end-to-end connectivity). <br>*Outcome:* Able to create an account and see a placeholder dashboard page. | ~2 weeks |
| **2. Company Config & Knowledge Base MVP** <br>*Tenant setup and content management* | - **Company CRUD:** Extend backend to support creating a Company (either auto-created on signup or via an API). Allow updating company settings (domain, name). <br>- **File Storage:** Decide storage for knowledge docs (likely Firestore). Initialize Firebase integration (admin SDK in Django) and ensure backend can write/read. <br>- **Knowledge Base API:** Implement endpoints to add, list, delete knowledge base entries. Initially, support plain text input or a textarea for FAQs. <br>- **Dashboard UI:** Build React components for updating company info (branding, domain) and for adding Q&A pairs or pasting support text. <br>- **Embedding Generation:** Integrate an embedding generation call in the backend when a new doc is added. Possibly call OpenAI’s embed API (store API key in config). Verify we can store embedding vectors in Firestore. If embedding seems too complex for now, we might skip and use simple keyword search as a placeholder, but given the importance, we aim to include it. <br>*Outcome:* Admin users can log in, set their company settings, and upload some support text. Data gets stored in Firestore. | ~3 weeks |
| **3. Basic Q&A Chatbot Functionality** <br>*Single-turn Q&A via API* | - **Chat API (non-streaming):** Implement a basic `/api/chat` that takes a question, does retrieval from Firestore (if embeddings ready, use them; if not, maybe fallback to keyword search or return a dummy answer), and calls OpenAI to get an answer. At this stage, focus on single-turn (no conversation history yet). <br>- **OpenAI Integration:** Write code to call the Chat Completion API with a simple prompt structure (system message with one piece of context for now). Use company’s first knowledge doc as context for testing. <br>- **Return Answer:** Have the API return the answer text. <br>- **Test Tool:** Create a simple test interface in the admin dashboard where the user can input a question and see the bot’s answer (this helps verify the pipeline). <br>- **Tone & Prompting:** Incorporate tone preference from company settings into the system prompt (e.g. if tone is set to “friendly”). <br>- **Knowledge Limits:** If no knowledge base data, decide on response (likely “I don’t know”). <br>*Outcome:* We have the chatbot working in a rudimentary way via the backend. Admins can test ask a question in the dashboard and get a response based on their provided data. | ~3 weeks |
| **4. Embeddable Widget Development** <br>*Web integration* | - **Widget UI:** Build the chat widget HTML/CSS/JS. It can be a separate React component or pure JS. Start with a basic UI: a toggleable chat window with an input field. Style it with some default theme. <br>- **Embed Script:** Create the embed code snippet. Possibly an HTML template the backend populates with the company’s API key. Or host a static `widget.js` that can fetch config by company ID. We might start with a simple approach: a static `widget.js` that expects a global `companyId` variable. <br>- **Communication:** Make the widget call the backend’s chat API (the one from milestone 3). At first, we can do a regular POST and await the full answer (we will add streaming later). <br>- **Demo on Test Page:** Create a dummy static site or just use the admin site for testing – embed the widget snippet in a page and verify that it loads and can talk to the backend and display an answer. <br>- **Branding:** Use the company config (maybe have the widget call an endpoint like `/api/bot/config` to get brand color and greeting). Apply the color to widget CSS (e.g. button color) and display a greeting message when opened. <br>- **Domain Check (Dev mode):** For now, possibly disable strict domain checks to ease testing on localhost. But ensure the infrastructure is in place to later enforce it. <br>*Outcome:* A functional chat widget that can be embedded on a webpage and returns answers. Not streaming yet, and minimal security, but proves the concept. | ~4 weeks |
| **5. Multi-turn Conversations & Streaming** <br>*Conversation state and improved UX* | - **Conversation Tracking:** Update the chat API to handle a `conversation_id`. Implement the logic to create conversation records in Firestore and message records. <br>- **Context Inclusion:** Modify the prompt construction to include previous messages (for now, maybe just the last exchange to conserve tokens). Test that follow-up questions now work (e.g. ask “What is warranty?” then “How long does it last?” and ensure second answer knows “warranty” context). <br>- **Widget Session:** Update widget to handle conversation IDs: store the `conversation_id` from the first response (backend can include it in response JSON), and send it with subsequent messages. Also keep messages in UI state so they persist in the widget view. <br>- **Streaming Response:** Switch the backend chat API to streaming mode. Use Django’s SSE capability to send chunks. Update the widget to listen for SSE (use EventSource or fetch streams). This will likely involve changing the API from POST to a GET or upgrading the HTTP connection (we might use a GET that takes query params like `?conversation_id=XYZ&message=Hello` to initiate SSE). Alternatively, use WebSockets (which is more complex); SSE is simpler to implement quickly. Ensure the widget can assemble the chunks into a final message. <br>- **Typing Indicator:** As soon as user sends a message, widget shows “Bot is typing…” indicator (and maybe starts the SSE connection). This provides immediate feedback. <br>- **Error Handling:** Handle if SSE disconnects or no response comes (timeout after e.g. 15 seconds and show error). <br>*Outcome:* Conversations are now stateful and the user experience is improved with streaming answers. The chatbot can handle follow-up questions within a session. | ~4 weeks |
| **6. Analytics & Admin Polishing** <br>*Logging and monitoring features* | - **Conversation Logs:** Ensure all conversations and messages are being saved properly to Firestore. Build an admin UI page to list conversations (at least a simple table with timestamp and maybe first user question or last interaction, and whether an email was collected). <br>- **Analytics Summary:** Implement a basic aggregation: e.g. count of conversations in the last 7 days. This could be computed on the fly by querying Firestore with a date filter, or by maintaining counters. For MVP, a simple query might be fine (Firestore can query by date and count results in code). Display this on an Analytics card. <br>- **Unanswered Queries:** Filter conversations or messages to find any where the bot gave a fallback (we might tag those conversations with a flag). List those for the admin so they can follow up. <br>- **UI/UX Improvements:** Refine the admin UI based on testing – e.g. form validations, better loading spinners, etc. Also refine the widget UI (make it more polished, add ability to scroll through chat history within the widget, etc.). Possibly add the company’s logo or name at the top of widget. <br>- **Documentation:** In the admin interface or as a separate doc, provide instructions for embedding the widget (so customers know how to paste the code). Could be a Markdown in the dashboard or a link. <br>*Outcome:* Admin users can see usage data and conversation histories, making the platform immediately useful to track their support bot’s performance. The overall UI is refined for a smoother experience. | ~3 weeks |
| **7. Security & Hardening** <br>*MVP stabilization* | - **Rate Limiting:** Implement IP-based rate limiting on the chat API. Possibly integrate `django-ratelimit` or DRF throttle classes. Test by hitting API rapidly to ensure it starts rejecting with 429 status. Also implement a simple exponential backoff mechanism: e.g., if an IP hits limit X times, temporarily block for Y minutes (we can store a timestamp in cache when to lift the block). ([API Rate Limits Explained: Best Practices for 2025 - Orq.ai](https://orq.ai/blog/api-rate-limit#:~:text=API%20Rate%20Limits%20Explained%3A%20Best,reaching%20the%20API%20rate%20limit)) <br>- **Domain Enforcement:** Now turn on the domain allowlist check for the widget API calls. Utilize the company’s `domain_allowlist` from Firestore. Test that if the widget is embedded on an unauthorized domain (simulate by changing origin header) it gets denied. Also configure CORS properly so that only those origins can call. <br>- **API Key/Secret:** Generate a random API key for each company (maybe at company creation) to use in embed. Update embed code to include it (or include it as a header). Backend verifies it. Essentially, treat this as a simple shared secret for the widget. <br>- **Prompt Injection Safeguards:** Add input sanitization on user questions: e.g. strip out any suspicious phrases or at least log them. Possibly maintain a `blocklist` of words (like “ignore all previous instructions” or certain profanity) ([A Guide to Prompt Injection - Techniques and Preventive Measures. | by Vishesh Grover | Medium](https://medium.com/@visheshgrover0/a-guide-to-prompt-injection-techniques-and-preventive-measures-d00a28a30e60#:~:text=def%20response_checker%28response%29%3A%20,cannot%20be%20processed%20due%20to)). If detected, either modify the prompt (remove that part) or refuse. Also adjust the system prompt to explicitly ignore user attempts to change rules. <br>- **Testing Security:** Do a round of testing: try to break the chatbot (e.g. prompt it to reveal the system message), try to use one company’s credentials to access another’s data, etc., and ensure those are prevented. <br>- **Performance Test:** Simulate multiple concurrent chats (maybe with a script) to see if any issues arise (like Firestore contention or memory spikes). Optimize if needed (increase instance size or add caching as noted). <br>*Outcome:* The platform is secured for initial users – reasonable protection against misuse and data leaks. We’re ready to onboard a few beta customers safely. | ~3 weeks |
| **8. MVP Launch** <br>*Final review and deployment* | - **End-to-End Testing:** Final integration tests covering signup to embedding. Possibly recruit a pilot company or internal testers to integrate the widget on a test site and gather feedback. <br>- **Bug Fixes:** Fix any issues discovered in testing (UI glitches, edge errors, etc.). <br>- **Documentation & Deployment:** Write basic documentation for the platform (how to use dashboard, how to embed, what the chatbot can/can’t do). Set up monitoring on production (Firebase dashboard, AWS CloudWatch for server, etc.). <br>- **Launch MVP:** Move the backend to a stable production environment on AWS (maybe behind a custom domain with SSL). Ensure Vercel domain is set (custom domain like app.company.com). And then open up for real signups (if desired) or limited beta invites. <br>*Outcome:* MVP is live in production with initial users. |

*(The timeline above is an estimate; actual development might adjust based on team size and feedback between milestones.)*

Each milestone builds on the previous, and by Milestone 5-6 we have a usable product (which could be beta-tested), while 7 ensures it’s secure enough for real usage. After MVP, further enhancements could include: more complex analytics, support for more file types, multi-language support, a refined ML model or on-prem option, billing subsystem for a SaaS model, etc. But for the MVP scope given, this roadmap covers all critical features.

## Conclusion

By following this architecture and plan, we will implement a robust customer support chatbot platform with a Django backend, React frontend, Firebase data store, and integrated AI capabilities. The design emphasizes modularity (separating config, chat, analytics), security (tenant isolation, domain lock-down, rate limiting), and performance (fast widget, streaming responses). This comprehensive plan ensures that the resulting platform will meet the requirements and provide a solid foundation for future growth and refinement. 

Throughout development, we’ll keep testing with real-world scenarios and adjust as needed. With this approach, the user will have an MVP where companies can self-serve to create an AI support assistant that is secure, accurate, and easy to deploy on their websites, all within a few months of focused development. 

**Sources:**

- Firebolt Blog – *How We Built a Domain-Specific Support Chatbot Using RAG* (example of retrieval-augmented answers) ([How We Built a Domain-Specific Support Chatbot Using Firebolt with RAG](https://www.firebolt.io/blog/how-we-built-a-domain-specific-support-chatbot-using-firebolt-with-rag#:~:text=TL%3BDR%3A%20Firebolt%20built%20a%20support,latency%20answers))  
- Citus Data – *Multi-Tenancy with Django* (importance of scoping queries by tenant) ([Evolving django-multitenant to build scalable SaaS apps on Postgres & Citus - Citus Data](https://www.citusdata.com/blog/2023/05/09/evolving-django-multitenant-to-build-scalable-saas-apps-on-postgres-and-citus/#:~:text=Citus%20open%20source%20or%20on,multitenant%20easy%20for%20you))  
- PromptEngineering.org – *System Prompts in LLMs* (using system messages for persona/tone) ([The Difference Between System Messages and User ... - PromptHub](https://www.prompthub.us/blog/the-difference-between-system-messages-and-user-messages-in-prompt-engineering#:~:text=The%20Difference%20Between%20System%20Messages,on%20specific%20tasks%20and))  
- IBM Security – *Preventing Prompt Injection Attacks* (input validation & monitoring reduce injection risk) ([Protect Against Prompt Injection | IBM](https://www.ibm.com/think/insights/prevent-prompt-injection#:~:text=organizations%20can%20significantly%20mitigate%20the,in%20the%20loop%2C%20and%20more))  
- TomDekan.com – *Streaming LLM responses in Django via SSE* (using Django `StreamingHttpResponse` for token streams) ([Stream AI chats using Django in 5 minutes (OpenAI and Anthropic)  - Tom Dekan](https://tomdekan.com/articles/llm-stream#:~:text=No%20third))  
- Orq.ai – *API Rate Limits Explained 2025* (exponential backoff recommendation for repeated requests) ([API Rate Limits Explained: Best Practices for 2025 - Orq.ai](https://orq.ai/blog/api-rate-limit#:~:text=API%20Rate%20Limits%20Explained%3A%20Best,reaching%20the%20API%20rate%20limit))  
- Medium (Ali Jakhar) – *OpenAI Chat Context and Token Limit* (need for developers to maintain conversation context) ([How To Maintain OpenAi Chat Context Within Token Limit Production Level Part-1 | by Ali Jakhar | Medium](https://alijakhar.medium.com/how-to-maintain-openai-chat-context-within-token-limit-production-level-part-1-046eeb76ee8d#:~:text=Chat%20context%20is%20like%20the,below%20for%20a%20better%20understanding))

